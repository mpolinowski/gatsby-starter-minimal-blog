---
date: "2019-03-15"
title: "OKD Baremetal Triple Host Cluster"
categories:
  - LINUX
  - Docker
  - OpenShift
---

![Harbin, China](./photo-84356dfgd_67gfh6dfdj_o.jpg)



## Preparations

The goal of this tutorial ist to setup 3 CentOS7 node server for an OpenShift cluster:

* Master Node: The brain of the operation
* Infra Node:
* Worker Node: That will handle our app containers

First we need to make sure that all 3 systems are up-to-date by logging in as `root` user and running the update command:


```bash
sudo su
yum update -c
```


### SELinux

Then make sure that [SELinux](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/deployment_guide/ch-selinux) is enabled on all nodes `SELINUX=enforcing`:


![OKD Origin Kubernetes Distribution](./okd_baremetal_01.png)


```bash
nano /etc/sysconfig/selinux
reboot
```

You can verify that SELinux is enabled with `sestatus`.


### hostnames

Now we want to make sure that all 3 nodes are able to contact each other via a hostname instead of an IP address. For this we have to edit each nodes hostname `nano /etc/hostname` and add those hostnames to the hosts file with the corresponding WAN IP address of each node `nano /etc/hosts`: 


![OKD Origin Kubernetes Distribution](./okd_baremetal_02.png)

![OKD Origin Kubernetes Distribution](./okd_baremetal_03.png)

You can check each hosts hostname and WAN IP with the `hostname` and `hostname -I` commands. Then assign the corresponding WAN IP to each hostname like:


```conf
# OpenShift Cluster
111.45.26.3 in-centos-master master
111.45.26.5 in-centos-infra infra  
165.122.1.174 in-centos-minion minion 
```

### Install Dependencies

Now we need to set up the base for our cluster on every node:


```bash
yum install -y wget gettext figlet git zile nano net-tools docker-1.13.1 bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openssl-devel httpd-tools NetworkManager python-cryptography python2-pip python-devel python-passlib java-1.8.0-openjdk-headless "@Development Tools" epel-release
```


Install EPEL and disable EPEL repository globally to prevent them from being installed accidentally and start/enable the Network Network Manager as well as Docker by executing the following shell script - `nano setup.sh`, `chmod +x setup.sh` and `./setup.sh`:


```sh
# Disable the EPEL repository globally so that is not accidentally used during later steps of the installation
sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo

systemctl | grep "NetworkManager.*running"
if [ $? -eq 1 ]; then
        systemctl start NetworkManager
        systemctl enable NetworkManager
fi

systemctl restart docker
systemctl enable docker
```


## OpenShift Master

1. Install the packages for Ansible

```bash
yum -y --enablerepo=epel install ansible pyOpenSSL
curl -o ansible.rpm https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm
yum -y --enablerepo=epel install ansible.rpm
```


2. Checkout openshift-ansible repository

```bash
git clone https://github.com/openshift/openshift-ansible.git
cd openshift-ansible && git fetch && git checkout release-3.11 && cd ..
```


3. Openshift Credentials

```bash
mkdir -p /etc/origin/master/
touch /etc/origin/master/htpasswd
```


4. Enable SSH Communication between the Master and other nodes


![OKD Origin Kubernetes Distribution](./okd_baremetal_04.png)


```bash
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub | ssh root@195.201.148.210 -p 45454 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys"
cat ~/.ssh/id_rsa.pub | ssh root@116.203.51.18 -p 56721 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys"
cat ~/.ssh/id_rsa.pub | ssh root@78.46.145.148 -p 35745 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys"
```



```ini
[OSEv3:children]
masters
nodes
etcd

[masters]
195.201.148.210 openshift_ip=195.201.148.210 openshift_schedulable=true openshift_shedulable=true ansible_connection=local ansible_become=yes

[etcd]
195.201.148.210 openshift_ip=195.201.148.210 ansible_connection=local ansible_become=yes

[nodes]
195.201.148.210 openshift_ip=195.201.148.210 openshift_schedulable=true openshift_node_group_name='node-config-master'
78.46.145.148 openshift_ip=78.46.145.148 openshift_schedulable=true ansible_become=yes ansible_sudo_pass=<password> ansible_user=<username> ansible_ssh_pass=<password> openshift_node_group_name='node-config-compute'
116.203.51.18 openshift_ip=116.203.51.18 openshift_schedulable=true ansible_become=yes ansible_sudo_pass=<password> ansible_user=<username> ansible_ssh_pass=<password> openshift_node_group_name='node-config-infra'

[OSEv3:vars]
openshift_additional_repos=[{'id': 'centos-paas', 'name': 'centos-paas', 'baseurl' :'https://buildlogs.centos.org/centos/7/paas/x86_64/openshift-origin311', 'gpgcheck' :'0', 'enabled' :'1'}]

ansible_ssh_user=root
enable_excluders=False
enable_docker_excluder=False
ansible_service_broker_install=False

containerized=True
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability

deployment_type=origin
openshift_deployment_type=origin

template_service_broker_selector={"region":"infra"}
openshift_metrics_image_version="v3.11"
openshift_logging_image_version="v3.11"
openshift_logging_elasticsearch_proxy_image_version="v1.0.0"
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true"}
logging_elasticsearch_rollout_override=false
osm_use_cockpit=true

openshift_metrics_install_metrics=false
openshift_logging_install_logging=false

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_file='/etc/origin/master/htpasswd'

openshift_public_hostname=console.test.instar.wiki
openshift_master_default_subdomain=apps.test.instar.wiki

openshift_master_api_port=8443
openshift_master_console_port=8443
```




```sh
#!/bin/bash
#The below configuration can be edited up on your needs and and please note the it's just an example configuration.
#We are going to create an OKD cluster with one master and 3 worker nodes.

#OKD Version
export OKD_VERSION=3.11

#The  below setting will be used to access OKD console https://console.$DOMAIN:$API_PORT"
#By default we can login using the URL https://console.okd.nip.io:8443
#To access URL from your local system we need to configure master host in C:\Windows\System32\drivers\etc\hosts file as below
#100.10.10.100  console.okd.nip.io
export DOMAIN=test.instar.wiki
export API_PORT=8443

#OKD Login Credentials
#By default admin/admin operator will be created and can be used to login to OKD console.
export OKD_USERNAME=OKDtest2019
export OKD_PASSWORD=cAMQJLCK0Bbzxi6xJ6nxaVYxZOcaNECsxiyHJzeVbPOeYYM9

#OKD Add-Ons
#Enable "True"  only if one of the VM has 4GB memory.
export INSTALL_METRICS=False

# Enable "True"  only one of the VM 16GB memory. 
export INSTALL_LOGGING=False

envsubst < inventory.download > inventory.ini

# install the packages for Ansible
yum -y --enablerepo=epel install ansible pyOpenSSL
curl -o ansible.rpm https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm
yum -y --enablerepo=epel install ansible.rpm

# checkout openshift-ansible repository
[ ! -d openshift-ansible ] && git clone https://github.com/openshift/openshift-ansible.git
cd openshift-ansible && git fetch && git checkout release-${OKD_VERSION} && cd ..

mkdir -p /etc/origin/master/
touch /etc/origin/master/htpasswd

# check pre-requisites
ansible-playbook -i inventory.ini openshift-ansible/playbooks/prerequisites.yml

# deploy cluster
ansible-playbook -i inventory.ini openshift-ansible/playbooks/deploy_cluster.yml

htpasswd -b /etc/origin/master/htpasswd $OKD_USERNAME ${OKD_PASSWORD}
oc adm policy add-cluster-role-to-user cluster-admin $OKD_USERNAME


curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
chmod +x get_helm.sh
./get_helm.sh


kubectl --namespace kube-system create serviceaccount tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account tiller --upgrade


echo "#####################################################################"
echo "* Your console is https://console.$DOMAIN:$API_PORT"
echo "* Your username is $OKD_USERNAME "
echo "* Your password is $OKD_PASSWORD "
echo "*"
echo "* Login using:"
echo "*"
echo "$ oc login -u ${OKD_USERNAME} -p ${OKD_PASSWORD} https://console.$DOMAIN:$API_PORT/"
echo "#####################################################################"

oc login -u ${OKD_USERNAME} -p ${OKD_PASSWORD} https://console.$DOMAIN:$API_PORT/
```